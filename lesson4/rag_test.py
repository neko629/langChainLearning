from langchain_deepseek import ChatDeepSeek
from langchain_ollama import OllamaEmbeddings
from langchain_openai.embeddings import OpenAIEmbeddings
from dotenv import load_dotenv


load_dotenv(override = True)

model = ChatDeepSeek(model = "deepseek-chat", temperature = 0)

embeddings = OllamaEmbeddings(
    model = "bge-m3"
)

# test the model and embeddings
# response = model.invoke([{"role": "user", "content": "Hello, DeepSeek!"}])
# print("Model response:", response)
# embedding_vector = embeddings.embed_query("Test embedding")
# print("Embedding vector:", embedding_vector)

# load documents
from langchain_community.document_loaders import TextLoader, Docx2txtLoader

loader = TextLoader("sample_document.txt", encoding = "utf-8")
documents = loader.load()

sensitive_loader = TextLoader("sensitive_document.txt", encoding = "utf-8")
sensitive_documents = sensitive_loader.load()

# print(documents[0].page_content)
# print(sensitive_documents[0].page_content)

# split documents
from langchain_text_splitters import RecursiveCharacterTextSplitter

text_splitter = RecursiveCharacterTextSplitter(
    chunk_size = 500, # æ¯ä¸ªå—çš„æœ€å¤§å­—ç¬¦æ•°
    chunk_overlap = 50, # å—ä¹‹é—´çš„é‡å å­—ç¬¦æ•°
    separators = ["\n\n", "\n", " ", ""]
)

texts = text_splitter.split_documents(documents)

sensitive_texts = text_splitter.split_documents(sensitive_documents)

# print(f"after splitting, we have {len(texts)} chunks.")
# print(f"after splitting, we have {len(sensitive_texts)} sensitive chunks.")

# create vector store and query
from langchain_community.vectorstores import FAISS

vector_store = FAISS.from_documents(texts, embeddings)
vector_store.save_local("faiss_index")

vector_store = FAISS.load_local(
    "faiss_index",
    embeddings,
    allow_dangerous_deserialization = True # å…è®¸ä¸å®‰å…¨çš„ååºåˆ—åŒ–
)
print(f"save normal vector store successfully")

sensitive_vector_store = FAISS.from_documents(sensitive_texts, embeddings)
sensitive_vector_store.save_local("sensitive_faiss_index")

sensitive_vector_store = FAISS.load_local(
    "sensitive_faiss_index",
    embeddings,
    allow_dangerous_deserialization = True # å…è®¸ä¸å®‰å…¨çš„ååºåˆ—åŒ–
)
print(f"save sensitive vector store successfully")

# load and create retriever
from langchain_community.retrievers import BM25Retriever
from langchain_classic.retrievers import EnsembleRetriever

# create bm25 retriever
bm25_retriever = BM25Retriever.from_documents(texts)
bm25_retriever.k = 3 # è®¾ç½®è¿”å›çš„æ–‡æ¡£æ•°é‡

# create ensemble retriever
faiss_retriever = vector_store.as_retriever(
    search_type = "similarity",
    search_kwargs = {"k": 3} # è®¾ç½®è¿”å›çš„æ–‡æ¡£æ•°é‡
)

ensemble_retriever = EnsembleRetriever(
    retrievers = [faiss_retriever, bm25_retriever],
    weights = [0.5, 0.5] # è®¾ç½®å„ä¸ªæ£€ç´¢å™¨çš„æƒé‡
)

print("Retrievers created successfully")

sensitive_bm25_retriever = BM25Retriever.from_documents(sensitive_texts)
sensitive_bm25_retriever.k = 3

sensitive_faiss_retriever = sensitive_vector_store.as_retriever(
    search_type = "similarity",
    search_kwargs = {"k": 3}
)

sensitive_ensemble_retriever = EnsembleRetriever(
    retrievers = [sensitive_faiss_retriever, sensitive_bm25_retriever],
    weights = [0.5, 0.5]
)

print("Sensitive retrievers created successfully")

from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser

def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)

template = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„é—®ç­”åŠ©æ‰‹ã€‚è¯·æ ¹æ®ä»¥ä¸‹æä¾›çš„ä¸Šä¸‹æ–‡ä¿¡æ¯æ¥å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚
å¦‚æœä¸Šä¸‹æ–‡ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·è¯šå®åœ°å‘Šè¯‰ç”¨æˆ·ä½ ä¸çŸ¥é“ï¼Œä¸è¦ç¼–é€ ç­”æ¡ˆã€‚

ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼š
{context}

é—®é¢˜: {question}

å›ç­”:"""

prompt = ChatPromptTemplate.from_template(template)

chain = ensemble_retriever | format_docs # å®šä¹‰æ£€ç´¢é“¾, å…ˆæ£€ç´¢å†æ ¼å¼åŒ–æ–‡æ¡£

# retrieval = chain.invoke("Langchainæ˜¯ä»€ä¹ˆï¼Ÿ")
# print("Retrieval result:", retrieval)
# print("=" * 60)
#
# retrieval_chain = (
#     {"context": ensemble_retriever | format_docs, "question": RunnablePassthrough()}
#     | prompt | model | StrOutputParser()
# ) # å®šä¹‰å®Œæ•´çš„æ£€ç´¢é—®ç­”é“¾, å…ˆæ£€ç´¢å†å›ç­”
#
# content = retrieval_chain.invoke("Langchainæ˜¯ä»€ä¹ˆï¼Ÿ")
# print(f"Final answer:\n{content}")
# print("=" * 60)

from langchain_tavily import TavilySearch

web_search = TavilySearch(max_results = 2)
# search_result = web_search.invoke("ä»‹ç»ä¸€ä¸‹ LangChain è¿™ä¸ªæ¡†æ¶")
#
# print(search_result)

from pydantic import BaseModel, Field
from langchain_core.tools import StructuredTool

class QAWithRetrievalArgs(BaseModel):
    query: str = Field(..., description = "ç”¨æˆ·çš„æŸ¥è¯¢é—®é¢˜")

def query_retrieval_knowledge(query: str) -> str:
    """
    ä¸€ä¸ªåŸºäºLangChainçŸ¥è¯†åº“æ£€ç´¢çš„é—®ç­”å·¥å…·ã€‚
    ä¸“é—¨ç”¨äºå›ç­”ä¸ LangChain ç›¸å…³çš„æŠ€æœ¯é—®é¢˜ã€‚

    âš ï¸ é‡è¦ï¼šæ­¤å·¥å…·ä»…é€‚ç”¨äº LangChain ç›¸å…³é—®é¢˜ï¼
    å¦‚æœé—®é¢˜ä¸ LangChain æ— å…³ï¼Œè¯·ä½¿ç”¨ç½‘ç»œæœç´¢å·¥å…·ã€‚
    """
    # å®šä¹‰ LangChain ç›¸å…³å…³é”®è¯
    langchain_keywords = [
        'langchain', 'langgraph', 'langsmith', 'lcel',
        'chain', 'agent', 'retriever', 'embedding', 'vector',
        'rag', 'prompt', 'llm', 'chatmodel', 'runnable',
        'é“¾', 'ä»£ç†', 'æ£€ç´¢å™¨', 'å‘é‡', 'æç¤ºè¯', 'æ¨¡å‹'
    ]

    query_lower = query.lower()
    is_langchain_related = any(keyword in query_lower for keyword in langchain_keywords)

    # å¦‚æœé—®é¢˜ä¸ç›¸å…³ï¼Œæç¤ºç”¨æˆ·ä½¿ç”¨ç½‘ç»œæœç´¢
    if not is_langchain_related:
        return (
            "æ­¤é—®é¢˜ä¼¼ä¹ä¸ LangChain æ— å…³ã€‚è¯·ä½¿ç”¨ç½‘ç»œæœç´¢å·¥å…·(tavily_search_results_json)è·å–ç­”æ¡ˆã€‚\n"
            f"é—®é¢˜: {query}"
        )

    retrieval_chain = ensemble_retriever | format_docs
    docs = retrieval_chain.invoke(query)

    # check result length
    if not docs or len(docs.strip()) < 50:
        return (
            f"çŸ¥è¯†åº“ä¸­æœªæ‰¾åˆ°å…³äº'{query}'çš„å……åˆ†ä¿¡æ¯ã€‚\n"
            "å»ºè®®: è¯·ä½¿ç”¨ç½‘ç»œæœç´¢å·¥å…·(tavily_search_results_json)è·å–ç­”æ¡ˆã€‚"
        )

    return docs

qa_tool = StructuredTool.from_function(
    func = query_retrieval_knowledge, # ç”ŸæˆåŸºäºæ£€ç´¢çš„é—®ç­”å·¥å…·
    name = "query_retrieval_knowledge", # å·¥å…·åç§°
    description = (
        "ğŸ¯ ä¸“ç”¨äºå›ç­” LangChain æŠ€æœ¯ç›¸å…³é—®é¢˜çš„çŸ¥è¯†åº“æ£€ç´¢å·¥å…·ã€‚\n"
        "é€‚ç”¨èŒƒå›´ï¼šLangChainã€LangGraphã€LangSmithã€LCELã€Agentã€RAGã€Retrieverã€Embeddingã€Prompt ç­‰ç›¸å…³æŠ€æœ¯ã€‚\n"
        "âš ï¸ é™åˆ¶ï¼šä»…åŒ…å« LangChain ç›¸å…³æ–‡æ¡£ï¼Œä¸é€‚ç”¨äºå…¶ä»–é¢†åŸŸé—®é¢˜ï¼ˆå¦‚çƒ¹é¥ªã€å†å²ã€ç§‘å­¦ç­‰ï¼‰ã€‚\n"
        "å¦‚æœé—®é¢˜ä¸ LangChain æ— å…³ï¼Œè¯·ä½¿ç”¨ç½‘ç»œæœç´¢å·¥å…· tavily_search_results_jsonã€‚"
    ),
    args_schema = QAWithRetrievalArgs, # å‚æ•°æ¨¡å¼
    return_direct = False # ä¸ç›´æ¥è¿”å›å·¥å…·ç»“æœ
)

# result = qa_tool.invoke("LangChain æ˜¯ä»€ä¹ˆï¼Ÿ")
# print("QA Tool Result:\n", result)

# å®šä¹‰é«˜é£é™©çŸ¥è¯†åº“æ•æ„Ÿæ•°æ®æŸ¥è¯¢å·¥å…·
class SensitiveKnowledgeQueryArgs(BaseModel):
    query: str = Field(description="æŸ¥è¯¢çš„æ•æ„Ÿä¸»é¢˜æˆ–å…³é”®è¯")
    data_category: str = Field(
        description="æ•°æ®ç±»åˆ«ï¼šconfidential(æœºå¯†), internal(å†…éƒ¨), sensitive(æ•æ„Ÿ)",
        default="confidential"
    )

def query_sensitive_knowledge(query: str, data_category: str = "confidential") -> str:
    """
    âš ï¸ é«˜é£é™©æ“ä½œï¼šåŸºäº RAG çš„æ•æ„ŸçŸ¥è¯†åº“æ£€ç´¢

    ä½¿ç”¨å‘é‡æ£€ç´¢ + BM25 æ··åˆæ£€ç´¢æ•æ„Ÿæ–‡æ¡£ã€‚
    åŒ…å«æœºå¯†æ–‡æ¡£ã€å†…éƒ¨èµ„æ–™ã€æ•æ„Ÿä¿¡æ¯ç­‰ã€‚

    é£é™©ç­‰çº§ï¼šğŸ”´ é«˜é£é™©
    - è®¿é—®æœºå¯†æ–‡æ¡£å’Œæ•æ„Ÿä¿¡æ¯
    - å¯èƒ½æ¶‰åŠå•†ä¸šæœºå¯†ã€ä¸ªäººéšç§
    - éœ€è¦æƒé™éªŒè¯å’Œäººå·¥å®¡æ ¸æ‰¹å‡†
    """

    print(f"\nğŸ”´ [é«˜é£é™©æ“ä½œ] æ•æ„ŸçŸ¥è¯†åº“ RAG æ£€ç´¢")
    print(f"   æ•°æ®ç±»åˆ«: {data_category}")
    print(f"   æŸ¥è¯¢å†…å®¹: {query}")

    sensitive_categories = {
        "confidential": "ğŸ”´ æœºå¯†çº§",
        "internal": "ğŸŸ¡ å†…éƒ¨çº§",
        "sensitive": "ğŸŸ  æ•æ„Ÿçº§"
    }

    category_label = sensitive_categories.get(data_category, "æœªçŸ¥çº§åˆ«")

    print(f"    æ­£åœ¨æ£€ç´¢ {category_label} æ•æ„Ÿæ–‡æ¡£...")
    retrieval_chain = sensitive_ensemble_retriever | format_docs
    docs = retrieval_chain.invoke(query)

    # æ£€æŸ¥æ£€ç´¢ç»“æœè´¨é‡
    if not docs or len(docs.strip()) < 50:
        return (
            f"âš ï¸ æ•æ„ŸçŸ¥è¯†åº“ä¸­æœªæ‰¾åˆ°å…³äº '{query}' çš„ç›¸å…³ä¿¡æ¯ã€‚\n"
            f"æ•°æ®ç±»åˆ«ï¼š{category_label}\n"
            f"æç¤ºï¼šè¯·ç¡®è®¤æŸ¥è¯¢å…³é”®è¯æ˜¯å¦å‡†ç¡®ï¼Œæˆ–å°è¯•ä½¿ç”¨ä¸åŒçš„å…³é”®è¯ã€‚\n"
            f"å¯æŸ¥è¯¢çš„ç±»åˆ«ï¼šæœºå¯†(confidential)ã€å†…éƒ¨(internal)ã€æ•æ„Ÿ(sensitive)"
        )

    output = f"{category_label} æ£€ç´¢ç»“æœ\n"
    output += "=" * 70 + "\n\n"
    output += "ğŸ“‹ æ£€ç´¢åˆ°çš„æ•æ„Ÿä¿¡æ¯ï¼š\n\n"
    output += docs
    output += "\n\n" + "=" * 70
    output += f"\n\nâš ï¸ å®‰å…¨è­¦å‘Šï¼š\n"
    output += f"- ä»¥ä¸Šä¸º{category_label}ä¿¡æ¯ï¼Œè¯·å¦¥å–„ä¿ç®¡ï¼Œä¸å¾—å¤–æ³„ï¼\n"
    output += f"- è®¿é—®å·²è®°å½•ï¼Œå°†ç”¨äºå®‰å…¨å®¡è®¡\n"
    output += f"- å¦‚éœ€åˆ†äº«ï¼Œè¯·ç¡®ä¿æ¥æ”¶æ–¹å…·æœ‰ç›¸åº”æƒé™\n"
    output += f"- æŸ¥è¯¢æ—¶é—´ï¼š{__import__('datetime').datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"

    return output

sensitive_knowledge_tool = StructuredTool.from_function(
    func = query_sensitive_knowledge,
    name = "query_sensitive_knowledge",
    description = (
        "ğŸ”´ é«˜é£é™©æ“ä½œï¼šæ•æ„ŸçŸ¥è¯†åº“æŸ¥è¯¢å·¥å…·\n"
        "ç”¨äºæŸ¥è¯¢çŸ¥è¯†åº“ä¸­çš„æœºå¯†æ–‡æ¡£ã€å†…éƒ¨èµ„æ–™ã€æ•æ„Ÿä¿¡æ¯ç­‰å—é™æ•°æ®ã€‚\n"
        "âš ï¸ è­¦å‘Šï¼šæ­¤æ“ä½œéœ€è¦äººå·¥å®¡æ ¸æ‰¹å‡†ï¼\n"
        "é€‚ç”¨åœºæ™¯ï¼š\n"
        "- æŸ¥è¯¢è´¢åŠ¡æ•°æ®ã€æˆ˜ç•¥è§„åˆ’ç­‰æœºå¯†ä¿¡æ¯\n"
        "- è®¿é—®æŠ€æœ¯æ–‡æ¡£ã€äººäº‹ä¿¡æ¯ç­‰å†…éƒ¨èµ„æ–™\n"
        "- è·å–ç”¨æˆ·æ•°æ®ã€å®¢æˆ·ä¿¡æ¯ç­‰æ•æ„Ÿæ•°æ®\n"
        "å®‰å…¨æç¤ºï¼šä»…åœ¨å¿…è¦æ—¶ä½¿ç”¨ï¼Œç¡®ä¿æœ‰ç›¸åº”æƒé™ã€‚"
    ),
    args_schema = SensitiveKnowledgeQueryArgs,
    return_direct = False
)

# result = sensitive_knowledge_tool.invoke("æŸ¥è¯¢ä¸€ä¸‹ 2024 å¹´ Q4 è´¢åŠ¡æŠ¥å‘Šæ•°æ®")
# print(f"Sensitive Knowledge Tool Result:\n{result}")

# agent execution
from typing import TypedDict
from langchain.agents import create_agent
from langgraph.checkpoint.memory import InMemorySaver

class Context(TypedDict):
    user_role: str

tools = [qa_tool, web_search, sensitive_knowledge_tool]

config = {"configurable": {"thread_id": "rag_test_user_001"}}

agent = create_agent(
    model = model,
    tools = tools,
    checkpointer = InMemorySaver(),
    context_schema = Context,
    debug = False
)

# for chunk in agent.stream(
#        # {"messages": [{"role": "user", "content": "Langchain æ”¯æŒé‚£äº›æ¨¡å‹?"}]}, # å•å·¥å…·é—®é¢˜
#         {"messages": [{"role": "user", "content": "æ¯”è¾ƒRAGå’ŒAgentic RAGçš„åŒºåˆ«ï¼Œå¹¶æ¨èä½¿ç”¨åœºæ™¯"}]}, # å¤åˆå·¥å…·é—®é¢˜
#         context = {"user_role": "å¤§æ¨¡å‹å·¥ç¨‹å¸ˆ"},
#         config = config,
#         stream_mode = "values"
# ):
#     if "messages" in chunk:
#         last_msg = chunk["messages"][-1]
#         if last_msg.type == "ai":
#             if hasattr(last_msg, 'tool_calls') and last_msg.tool_calls:
#                 tool_call = last_msg.tool_calls[0]
#                 print(f"[AI æ€è€ƒ]: å†³å®šè°ƒç”¨å·¥å…· -> {tool_call['name']}")
#                 print(f"args: {tool_call.get('args', {})}")
#             elif last_msg.content:
#                 print(f"[AI å›ç­”]: {last_msg.content}")

# ä¸Šä¸‹æ–‡å‹ç¼©ä¸­é—´ä»¶ before_model
from langchain.agents.middleware import SummarizationMiddleware

summarization_middleware = SummarizationMiddleware(
    model=ChatDeepSeek(model="deepseek-chat", temperature=0.1),    # æ‘˜è¦æ¨¡å‹
    trigger = [("messages", 5),  ("tokens", 200)],
    summary_prompt="è¯·å°†ä»¥ä¸‹å¯¹è¯å†å²è¿›è¡Œæ‘˜è¦ï¼Œä¿ç•™å…³é”®å†³ç­–ç‚¹å’ŒæŠ€æœ¯ç»†èŠ‚ï¼š\n\n{messages}\n\næ‘˜è¦:"  # æ‘˜è¦æç¤º
)

# è‡ªåŠ¨å·¥å…·é‡è¯•ä¸­é—´ä»¶ wrap_tool_call
from langchain.agents.middleware import ToolRetryMiddleware
retry_middleware = ToolRetryMiddleware(
    max_retries = 3,
    tools = tools,
    retry_on = (ConnectionError, RuntimeError),
    on_failure = "return_message", # å¤±è´¥åè¿”å›æ¶ˆæ¯
    backoff_factor = 1.5, # æŒ‡æ•°é€€é¿å› å­, æ¯æ¬¡é‡è¯•ç­‰å¾…æ—¶é—´å¢åŠ 1.5å€
)

# Tool è°ƒç”¨æ—¥å¿—ä¸­é—´ä»¶ (after_model)
import json
import time
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional, Callable
from langchain.agents.middleware import AgentMiddleware, ModelRequest, ModelResponse,   AgentState

class ToolCallLogger:
    """å·¥å…·è°ƒç”¨æ—¥å¿—è®°å½•å™¨"""

    def __init__(self, log_dir: str = "LangChain_AgenticRAG/logs"):
        self.log_dir = Path(log_dir)
        self.log_dir.mkdir(parents=True, exist_ok=True)
        self.current_session_logs: List[Dict[str, Any]] = []
        self.session_start_time = datetime.now()
        self.tool_call_times: Dict[str, float] = {}  # è®°å½•å·¥å…·è°ƒç”¨å¼€å§‹æ—¶é—´

        # Token ä½¿ç”¨ç»Ÿè®¡
        self.total_input_tokens = 0
        self.total_output_tokens = 0
        self.total_tokens = 0
        self.cache_hit_tokens = 0

    def get_log_file_path(self) -> Path:
        """è·å–å½“å‰æ—¥å¿—æ–‡ä»¶è·¯å¾„"""
        date_str = datetime.now().strftime("%Y%m%d")
        return self.log_dir / f"tool_calls_{date_str}.json"

    def log_tool_call(
        self,
        tool_name: str,
        tool_input: Any,
        tool_output: Any,
        success: bool,
        error: Optional[str] = None,
        metadata: Optional[Dict[str, Any]] = None,
        token_usage: int = 0,
    ):
        """è®°å½•å•æ¬¡å·¥å…·è°ƒç”¨"""
        log_entry = {
            "timestamp": datetime.now().isoformat(),
            "tool_name": tool_name,
            "input": str(tool_input)[:500],  # é™åˆ¶é•¿åº¦
            "output": str(tool_output)[:1000] if success else None,
            "success": success,
            "error": error,
            "metadata": metadata or {},
            "token_usage": token_usage,
        }

        self.current_session_logs.append(log_entry)

        # å®æ—¶å†™å…¥æ–‡ä»¶
        self._append_to_file(log_entry)

        # æ‰“å°æ—¥å¿—
        status = "âœ…" if success else "âŒ"
        if not success and error:
            print(f"   Error: {error}")

    def accumulate_tokens(
        self,
        input_tokens: int,
        output_tokens: int,
        total_tokens: int,
        cache_hit: int = 0
    ):
        """ç´¯è®¡ token ä½¿ç”¨é‡"""
        self.total_input_tokens += input_tokens
        self.total_output_tokens += output_tokens
        self.total_tokens += total_tokens
        self.cache_hit_tokens += cache_hit

        print(f"ğŸ“Š [Token Usage] è¾“å…¥: {input_tokens}, è¾“å‡º: {output_tokens}, æ€»è®¡: {total_tokens}")
        if cache_hit > 0:
            print(f"   ç¼“å­˜å‘½ä¸­: {cache_hit} tokens")

    def _append_to_file(self, log_entry: Dict[str, Any]):
        """è¿½åŠ æ—¥å¿—åˆ°æ–‡ä»¶"""
        log_file = self.get_log_file_path()

        # è¯»å–ç°æœ‰æ—¥å¿—
        if log_file.exists():
            with open(log_file, 'r', encoding='utf-8') as f:
                try:
                    logs = json.load(f)
                except json.JSONDecodeError:
                    logs = []
        else:
            logs = []

        # æ·»åŠ æ–°æ—¥å¿—
        logs.append(log_entry)

        # å†™å›æ–‡ä»¶
        with open(log_file, 'w', encoding='utf-8') as f:
            json.dump(logs, f, indent=2, ensure_ascii=False)

    def get_statistics(self) -> Dict[str, Any]:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        if not self.current_session_logs:
            return {"message": "No logs yet"}

        total_calls = len(self.current_session_logs)
        successful_calls = sum(1 for log in self.current_session_logs if log["success"])
        failed_calls = total_calls - successful_calls

        # ç»Ÿè®¡å·¥å…·ä½¿ç”¨æ¬¡æ•°
        tool_counts = {}
        for log in self.current_session_logs:
            tool_name = log["tool_name"]
            tool_counts[tool_name] = tool_counts.get(tool_name, 0) + 1

        return {
            "total_calls": total_calls,
            "successful_calls": successful_calls,
            "failed_calls": failed_calls,
            "success_rate": f"{(successful_calls/total_calls*100):.1f}%" if total_calls > 0 else "0%",
            "tool_usage": tool_counts,
            "token_usage": {
                "total_input_tokens": self.total_input_tokens,
                "total_output_tokens": self.total_output_tokens,
                "total_tokens": self.total_tokens,
                "cache_hit_tokens": self.cache_hit_tokens
            },
            "session_duration": str(datetime.now() - self.session_start_time)
        }

    def print_statistics(self):
        """æ‰“å°ç»Ÿè®¡ä¿¡æ¯"""
        stats = self.get_statistics()
        print("\n" + "="*70)
        print("ğŸ“Š Tool Call Statistics")
        print("="*70)
        for key, value in stats.items():
            print(f"  {key}: {value}")
        print("="*70)


class ToolLoggingMiddleware(AgentMiddleware):
    """
    åˆ›å»ºå·¥å…·æ—¥å¿—ä¸­é—´ä»¶
    ä½¿ç”¨ @wrap_model_call è£…é¥°å™¨ä» ModelRequest è·å–æ¶ˆæ¯å†å²
    """
    def __init__(self, log_dir: str = "LangChain_AgenticRAG/logs"):
        super().__init__()
        self.logger = ToolCallLogger()


    def after_model(self,state: AgentState, runtime) -> None:
        """
        ä» ModelRequest ä¸­è·å–æ¶ˆæ¯å†å²ï¼Œè®°å½•å·¥å…·è°ƒç”¨ä¿¡æ¯

        Args:
            request: ModelRequest åŒ…å« state (åŒ…æ‹¬ messages)
            handler: å¤„ç†å‡½æ•°ï¼Œæ‰§è¡Œå®é™…çš„æ¨¡å‹è°ƒç”¨

        Returns:
            ModelResponse æ¨¡å‹å“åº”
        """
        # ä» state è·å–æ¶ˆæ¯å†å²
        messages = state.get("messages", [])

        # print(f"ğŸ” [Tool Logging] åˆ†ææ¶ˆæ¯å†å²ï¼Œ{messages} æ¶ˆæ¯")

        # æ£€æŸ¥æ¶ˆæ¯å†å²ä¸­çš„å·¥å…·è°ƒç”¨å’Œç»“æœ
        for msg in messages:
            # æ£€æµ‹ AI æ¶ˆæ¯å¹¶æå– token ä½¿ç”¨ä¿¡æ¯
            if hasattr(msg, 'type') and msg.type == 'ai':
                # ä¼˜å…ˆä» usage_metadata è·å–
                if hasattr(msg, 'usage_metadata') and msg.usage_metadata:
                    input_tokens = msg.usage_metadata.get('input_tokens', 0)
                    output_tokens = msg.usage_metadata.get('output_tokens', 0)
                    total_tokens = msg.usage_metadata.get('total_tokens', 0)

                    # è·å–ç¼“å­˜å‘½ä¸­ä¿¡æ¯
                    cache_hit = 0
                    if 'input_token_details' in msg.usage_metadata:
                        cache_hit = msg.usage_metadata['input_token_details'].get('cache_read', 0)

                    # ç´¯è®¡ token
                    self.logger.accumulate_tokens(input_tokens, output_tokens, total_tokens, cache_hit)

                # å¤‡é€‰ï¼šä» response_metadata è·å–
                elif hasattr(msg, 'response_metadata') and msg.response_metadata:
                    token_usage = msg.response_metadata.get('token_usage', {})
                    if token_usage:
                        input_tokens = token_usage.get('prompt_tokens', 0)
                        output_tokens = token_usage.get('completion_tokens', 0)
                        total_tokens = token_usage.get('total_tokens', 0)
                        cache_hit = token_usage.get('prompt_cache_hit_tokens', 0)

                        # ç´¯è®¡ token
                        self.logger.accumulate_tokens(input_tokens, output_tokens, total_tokens, cache_hit)

            # æ£€æµ‹ AI æ¶ˆæ¯ä¸­çš„å·¥å…·è°ƒç”¨è¯·æ±‚
            if hasattr(msg, 'tool_calls') and msg.tool_calls:
                for tool_call in msg.tool_calls:
                    # tool_call å¯èƒ½æ˜¯å­—å…¸æˆ–å¯¹è±¡ï¼Œéœ€è¦å…¼å®¹ä¸¤ç§æ–¹å¼
                    if isinstance(tool_call, dict):
                        tool_name = tool_call.get('name', 'unknown')
                        tool_args = tool_call.get('args', {})
                        tool_id = tool_call.get('id', 'unknown_id')
                    else:
                        tool_name = getattr(tool_call, 'name', 'unknown')
                        tool_args = getattr(tool_call, 'args', {})
                        tool_id = getattr(tool_call, 'id', 'unknown_id')

                    # è®°å½•å·¥å…·è°ƒç”¨å¼€å§‹æ—¶é—´
                    if tool_id not in self.logger.tool_call_times:
                        self.logger.tool_call_times[tool_id] = time.time()
                        print(f"\nğŸ”§ [Tool Logging] æ£€æµ‹åˆ°å·¥å…·è°ƒç”¨: {tool_name}")
                        print(f"   å·¥å…·ID: {tool_id}")
                        print(f"   å‚æ•°: {str(tool_args)[:200]}...")

            # æ£€æµ‹å·¥å…·è¿”å›æ¶ˆæ¯
            if hasattr(msg, 'type') and msg.type == 'tool':
                tool_name = getattr(msg, 'name', 'unknown')
                tool_content = getattr(msg, 'content', '')
                tool_call_id = getattr(msg, 'tool_call_id', 'unknown_id')
                token_usage = getattr(msg, 'token_usage', 0)

                # åˆ¤æ–­æ˜¯å¦æˆåŠŸ
                success = not tool_content.startswith('âŒ') and not tool_content.startswith('Error')
                error_msg = tool_content if not success else None

                # è®°å½•æ—¥å¿—
                self.logger.log_tool_call(
                    tool_name=tool_name,
                    tool_input="[ä»æ¶ˆæ¯å†å²æå–]",
                    tool_output=tool_content,
                    success=success,
                    error=error_msg,
                    metadata={
                        "tool_call_id": tool_call_id,
                        "timestamp": datetime.now().isoformat(),
                        "message_type": msg.type
                    },
                    token_usage=token_usage
                )
        # æ‰“å°å½“å‰ç»Ÿè®¡ä¿¡æ¯
        self.logger.print_statistics()


# å®ä¾‹åŒ–æ—¥å¿—ä¸­é—´ä»¶
logging_middleware = ToolLoggingMiddleware(log_dir="./logs")

from langchain.agents.middleware import ToolCallLimitMiddleware
# å·¥å…·è°ƒç”¨é™åˆ¶ä¸­é—´ä»¶ after_model
retrieval_limit_middleware = ToolCallLimitMiddleware(
    tool_name="query_retrieval_knowledge",
    run_limit=3,  # æ¯æ¬¡è¿è¡Œæœ€å¤šè°ƒç”¨ 3 æ¬¡
    exit_behavior="continue"  # è¶…é™åç»§ç»­æ‰§è¡Œï¼Œä½†é˜»æ­¢å·¥å…·è°ƒç”¨
)

sensitive_limit_middleware = ToolCallLimitMiddleware(
    tool_name="query_sensitive_knowledge",
    run_limit=3,  # æ¯æ¬¡è¿è¡Œæœ€å¤šè°ƒç”¨ 3 æ¬¡
    exit_behavior="continue"  # è¶…é™åç»§ç»­æ‰§è¡Œï¼Œä½†é˜»æ­¢å·¥å…·è°ƒç”¨
)


# hilt äººå·¥ä»‹å…¥ä¸­é—´ä»¶ after_model
from langchain.agents.middleware import HumanInTheLoopMiddleware

official_hitl_middleware = HumanInTheLoopMiddleware(
    interrupt_on={"query_sensitive_knowledge": True},
    description_prefix="éœ€è¦äººå·¥æ‰¹å‡†æ‰èƒ½æŸ¥è¯¢æ•æ„ŸçŸ¥è¯†åº“"
)

# åŠ¨æ€æç¤ºè¯ä¸­é—´ä»¶ wrap_model_call
from langchain.agents.middleware import dynamic_prompt

@dynamic_prompt
def rag_optimized_prompt(request: ModelRequest) -> str:
    """
    æ ¹æ®æ£€ç´¢çŠ¶æ€åŠ¨æ€ç”Ÿæˆæç¤ºè¯
    æ ¸å¿ƒé€»è¾‘ï¼šé€šè¿‡åˆ†ææ¶ˆæ¯å†å²ä¸­çš„å·¥å…·è°ƒç”¨æ¬¡æ•°ï¼Œç¡®å®šå½“å‰æ‰€å¤„çš„ RAG é˜¶æ®µ
    """
    messages = request.messages if hasattr(request, 'messages') else []

    # ç»Ÿè®¡æ‰€æœ‰å·¥å…·è°ƒç”¨ä¸­çš„çŸ¥è¯†åº“æŸ¥è¯¢æ¬¡æ•°ï¼ˆåŒ…æ‹¬æ£€ç´¢å’Œæ•æ„ŸæŸ¥è¯¢ï¼‰
    retrieval_count = 0
    for msg in messages:
        if hasattr(msg, 'tool_calls') and msg.tool_calls:
            for tool_call in msg.tool_calls:
                name = tool_call.name if hasattr(tool_call, 'name') else tool_call.get('name')
                # ç»Ÿè®¡çŸ¥è¯†åº“æŸ¥è¯¢æ¬¡æ•°ï¼ˆåŒ…æ‹¬æ£€ç´¢å’Œæ•æ„ŸæŸ¥è¯¢ï¼‰
                if name == 'query_retrieval_knowledge' or name == 'tavily_search_results_json' or name == 'query_sensitive_knowledge': # é€šè¿‡æŠŠæŸ¥è¯¢åŒ…è£…æˆå·¥å…·, ç»Ÿè®¡è°ƒç”¨å·¥å…·çš„æ¬¡æ•°
                    retrieval_count += 1

    print(f"DEBUG: å½“å‰ç´¯è®¡æ£€ç´¢æ¬¡æ•°: {retrieval_count}")

    # åŸºç¡€æç¤ºè¯
    base_prompt = """ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½çŸ¥è¯†åŠ©æ‰‹ï¼Œèƒ½å¤Ÿè‡ªä¸»æ£€ç´¢ä¿¡æ¯å¹¶å›ç­”é—®é¢˜ã€‚

    ğŸ”§ å¯ç”¨å·¥å…·è¯´æ˜ï¼š
    1. query_retrieval_knowledge: ä¸“é—¨ç”¨äº LangChain æŠ€æœ¯é—®é¢˜ï¼ˆLangChainã€LangGraphã€Agentã€RAGã€Retriever ç­‰ï¼‰
    2. tavily_search_results_json: ç”¨äºé€šç”¨é—®é¢˜çš„ç½‘ç»œæœç´¢ï¼ˆçƒ¹é¥ªã€å†å²ã€ç§‘å­¦ã€æ–°é—»ç­‰ï¼‰
    3. query_sensitive_knowledge: ğŸ”´ é«˜é£é™©å·¥å…· - æŸ¥è¯¢æ•æ„ŸçŸ¥è¯†åº“ï¼ˆè´¢åŠ¡æ•°æ®ã€æˆ˜ç•¥è§„åˆ’ã€å®¢æˆ·ä¿¡æ¯ç­‰æœºå¯†èµ„æ–™ï¼‰

    âš ï¸ å·¥å…·é€‰æ‹©åŸåˆ™ï¼š
    - å¦‚æœé—®é¢˜æ¶‰åŠ LangChain ç›¸å…³æŠ€æœ¯ â†’ ä½¿ç”¨ query_retrieval_knowledge
    - å¦‚æœé—®é¢˜ä¸ LangChain æ— å…³ï¼ˆå¦‚çƒ¹é¥ªã€å†å²ã€ç§‘å­¦ç­‰ï¼‰ â†’ ç›´æ¥ä½¿ç”¨ tavily_search_results_json
    - å¦‚æœé—®é¢˜æ¶‰åŠæ•æ„Ÿæ•°æ®æŸ¥è¯¢ï¼ˆè´¢åŠ¡ã€æˆ˜ç•¥ã€å®¢æˆ·ã€äººäº‹ç­‰ï¼‰ â†’ ä½¿ç”¨ query_sensitive_knowledge
    - ä¸è¦å¯¹é LangChain é—®é¢˜è°ƒç”¨çŸ¥è¯†åº“æ£€ç´¢å·¥å…·

    ğŸ”´ é«˜é£é™©å·¥å…·ä½¿ç”¨æ³¨æ„äº‹é¡¹ï¼š
    - query_sensitive_knowledge éœ€è¦äººå·¥å®¡æ ¸æ‰¹å‡†æ‰èƒ½æ‰§è¡Œ
    - ä»…åœ¨ç”¨æˆ·æ˜ç¡®è¯·æ±‚æŸ¥è¯¢æœºå¯†/æ•æ„Ÿä¿¡æ¯æ—¶ä½¿ç”¨
    - è°ƒç”¨æ­¤å·¥å…·åï¼Œç³»ç»Ÿä¼šæš‚åœç­‰å¾…ç®¡ç†å‘˜æ‰¹å‡†
    - é€‚ç”¨åœºæ™¯ï¼šè´¢åŠ¡æŠ¥å‘Šã€æˆ˜ç•¥è§„åˆ’ã€å®¢æˆ·æ¡£æ¡ˆã€äººäº‹è–ªèµ„ã€æŠ€æœ¯æ–‡æ¡£ç­‰

    è¯·éµå¾ªä»¥ä¸‹æµç¨‹ï¼š
    1. åˆ†æç”¨æˆ·é—®é¢˜çš„ç±»å‹å’Œå¤æ‚åº¦
    2. åˆ¤æ–­é—®é¢˜æ˜¯å¦ä¸ LangChain ç›¸å…³ï¼Œæˆ–æ˜¯å¦æ¶‰åŠæ•æ„Ÿæ•°æ®
    3. é€‰æ‹©åˆé€‚çš„æ£€ç´¢å·¥å…·
    4. è¯„ä¼°æ£€ç´¢ç»“æœçš„è´¨é‡ï¼ˆè¦†ç›–ç‡ã€å®Œæ•´æ€§ã€ç›¸å…³æ€§ï¼‰
    5. å¦‚æœç»“æœä¸è¶³ï¼Œä¸»åŠ¨è¿›è¡Œè¡¥å……æ£€ç´¢
    6. ç»¼åˆæ‰€æœ‰ä¿¡æ¯ç”Ÿæˆæœ€ç»ˆå›ç­”
    """

    # åˆå§‹çŠ¶æ€ï¼šæœªè¿›è¡Œä»»ä½•çŸ¥è¯†åº“æŸ¥è¯¢
    if retrieval_count == 0:
        return base_prompt + """

        ã€å½“å‰çŠ¶æ€ï¼šåˆå§‹é˜¶æ®µã€‘
        âš ï¸ é‡è¦ï¼šä½ è¿˜æ²¡æœ‰è¿›è¡Œä»»ä½•æ£€ç´¢ï¼

        è¯·å…ˆåˆ¤æ–­é—®é¢˜ç±»å‹ï¼š
        - å¦‚æœæ˜¯ LangChain ç›¸å…³é—®é¢˜ â†’ ä½¿ç”¨ query_retrieval_knowledge
        - å¦‚æœæ˜¯å…¶ä»–é¢†åŸŸé—®é¢˜ â†’ ä½¿ç”¨ tavily_search_results_json
        - å¦‚æœæ¶‰åŠæ•æ„Ÿæ•°æ®æŸ¥è¯¢ â†’ ä½¿ç”¨ query_sensitive_knowledgeï¼ˆéœ€äººå·¥æ‰¹å‡†ï¼‰

        âŒ ç¦æ­¢åœ¨æ²¡æœ‰æ£€ç´¢çš„æƒ…å†µä¸‹ç›´æ¥å›ç­”é—®é¢˜ã€‚
        """

    # ä¿¡æ¯è¯„ä¼°é˜¶æ®µï¼šå·²è¿›è¡Œ 1-2 æ¬¡çŸ¥è¯†åº“æŸ¥è¯¢
    elif retrieval_count < 3:
        return base_prompt + f"""

        ã€å½“å‰çŠ¶æ€ï¼šä¿¡æ¯è¯„ä¼°ï¼ˆå·²æ£€ç´¢ {retrieval_count} æ¬¡ï¼‰ã€‘
        è¯·æ£€æŸ¥ä¸Šä¸€æ­¥å·¥å…·è¿”å›çš„æœç´¢ç»“æœï¼š
        1. ä¿¡æ¯æ˜¯å¦è¦†ç›–äº†ç”¨æˆ·é—®é¢˜çš„å…¨éƒ¨ç»´åº¦ï¼Ÿ
        2. å¤šä¸ªæ¥æºçš„ä¿¡æ¯æ˜¯å¦ä¸€è‡´ï¼Ÿ

        ğŸ‘‰ å†³ç­–è·¯å¾„ï¼š
        - å¦‚æœä¿¡æ¯ä¸è¶³æˆ–æœ‰æ­§ä¹‰ -> è¯·æ¢ä¸ªå…³é”®è¯æˆ–è§’åº¦è¿›è¡Œè¡¥å……æ£€ç´¢ã€‚
        - å¦‚æœä¿¡æ¯å·²ç»å……åˆ† -> è¯·æ ¹æ®ä¸Šä¸‹æ–‡ç”Ÿæˆæœ€ç»ˆå›ç­”ã€‚
        """

    # æœ€ç»ˆå›ç­”é˜¶æ®µï¼šå·²è¿›è¡Œ 3 æ¬¡åŠä»¥ä¸ŠçŸ¥è¯†åº“æŸ¥è¯¢
    else:
        return base_prompt + f"""

        ã€å½“å‰çŠ¶æ€ï¼šæœ€ç»ˆå›ç­”ï¼ˆå·²æ£€ç´¢ {retrieval_count} æ¬¡ï¼‰ã€‘
        ğŸ›‘ å·²è¾¾åˆ°æœ€å¤§æ£€ç´¢æ¬¡æ•°é™åˆ¶ï¼Œè¯·åœæ­¢æ£€ç´¢ï¼

        è¯·å¿…é¡»åŸºäºå½“å‰å·²æœ‰çš„æ‰€æœ‰ä¿¡æ¯ï¼Œç”Ÿæˆæœ€ç»ˆçš„å›ç­”ã€‚
        å¦‚æœæ£€ç´¢åˆ°çš„ä¿¡æ¯ä»ä¸èƒ½å®Œå…¨å›ç­”é—®é¢˜ï¼Œè¯·è¯šå®åœ°è¯´æ˜ä¿¡æ¯çš„å±€é™æ€§æˆ–ç¼ºå¤±éƒ¨åˆ†ã€‚
        """

# ä¸­é—´ä»¶é›†åˆ
middlewares = [
    # before_model: å‡†å¤‡é˜¶æ®µï¼Œä¸Šä¸‹æ–‡å‹ç¼©ä¸­é—´ä»¶
    summarization_middleware,

    # wrap_model_call: æ¨¡å‹è°ƒç”¨åŒ…è£¹ï¼Œæ™ºèƒ½åˆ‡æ¢ç³»ç»Ÿæç¤ºè¯
    rag_optimized_prompt,

    # after_model: åå¤„ç†ï¼ˆé€†åºæ‰§è¡Œï¼Œæ‰€ä»¥å€’ç€å†™ï¼‰
    official_hitl_middleware,  # æœ€åæ‰§è¡Œï¼šäººå·¥å®¡æ ¸ï¼ˆå¯èƒ½ä¸­æ–­ï¼‰
    logging_middleware,  # å€’æ•°ç¬¬äºŒï¼šè®°å½•æ—¥å¿—
    sensitive_limit_middleware,  # å€’æ•°ç¬¬ä¸‰ï¼šé™åˆ¶æ•æ„Ÿå·¥å…·
    retrieval_limit_middleware,  # æœ€å…ˆæ‰§è¡Œï¼šé™åˆ¶æ£€ç´¢å·¥å…·

    # wrap_tool_call: å·¥å…·è°ƒç”¨åŒ…è£¹
    retry_middleware,
]

from typing import TypedDict
from langchain.agents import create_agent
from langgraph.checkpoint.memory import InMemorySaver

# åˆ›å»ºå¹¶è¿è¡Œ Agent
class Context(TypedDict):
    user_role: str

config = {
    "configurable": {"thread_id": "rag_test_user_final"}
}

agent = create_agent(
    tools = tools,
    model = model,
    middleware = middlewares,
    debug = False,
    checkpointer = InMemorySaver(),
    context_schema = Context
)

# è§¦å‘ hitl ä¸­é—´ä»¶
from langgraph.types import Command
# å¯¼å…¥ HITL ç›¸å…³ç±»
from langchain.agents.middleware.human_in_the_loop import (
    HITLResponse,
    ApproveDecision,
    EditDecision,
    RejectDecision
)

def run_hitl_interactive_test():
    """
    è¿è¡Œäººå·¥ä»‹å…¥ä¸­é—´ä»¶æµ‹è¯•äº¤äº’ä¼šè¯
    å‚è€ƒ HITL_demo.py
    """
    print("\n" + "=" * 70)
    print("ğŸš€ å¼€å§‹æ‰§è¡Œ Agentic RAG æµ‹è¯• (HITL äººå·¥å¹²é¢„æ¨¡å¼)")
    print("=" * 70)

    # æµ‹è¯•æç¤ºè¯ï¼šè§¦å‘æ•æ„ŸçŸ¥è¯†åº“æŸ¥è¯¢
    user_input = "å¸®æˆ‘æŸ¥è¯¢ä¸€ä¸‹2024å¹´Q4è´¢åŠ¡æŠ¥å‘Šæ•°æ®çš„è¯¦ç»†å†…å®¹ã€‚"
    print(f"\n[ç”¨æˆ·]: {user_input}")

    print("\n[ç³»ç»Ÿ]: å¼€å§‹å¤„ç†è¯·æ±‚...")

    for event in agent.stream(
        {
            "messages": [{"role": "user", "content": user_input}]
        },
        config = config,
        stream_mode = "values",
        context = {"user_role": "è´¢åŠ¡åˆ†æå¸ˆ"}
    ):
        if "messages" in event:
            last_msg = event["messages"][-1]
            if last_msg.type == "ai" and hasattr(last_msg, "tool_calls") and last_msg.tool_calls:
                print(f"[AI å†³ç­–]: å‡†å¤‡è°ƒç”¨å·¥å…· -> {last_msg.tool_calls[0]['name']}")

    # è§‚å¯Ÿä¸­æ–­çŠ¶æ€
    snapshot = agent.get_state(config)

    print(f"\n--- ğŸ›‘ æ‰§è¡Œå·²æš‚åœ (HITL Middleware è§¦å‘) ---")
    print(f"ä¸‹ä¸€æ­¥éª¤: {snapshot.next}")
    print(f"ä»»åŠ¡æ•°é‡: {len(snapshot.tasks) if snapshot.tasks else 0}")

    if snapshot.tasks:
        last_message = snapshot.values["messages"][-1]

        if hasattr(last_message, "tool_calls") and last_message.tool_calls:
            tool_call = last_message.tool_calls[0]

            print(f"\n{'=' * 70}")
            print("ğŸ”´ æ£€æµ‹åˆ°é«˜é£é™©æ“ä½œï¼šæ•æ„ŸçŸ¥è¯†åº“æŸ¥è¯¢")
            print(f"{'=' * 70}")
            print(f"å·¥å…·åç§°: {tool_call['name']}")
            print(f"æŸ¥è¯¢å†…å®¹: {tool_call['args'].get('query', 'N/A')}")
            print(f"æ•°æ®ç±»åˆ«: {tool_call['args'].get('data_category', 'confidential')}")
            print(f"{'=' * 70}")

            # === ç¬¬ä¸‰æ­¥ï¼šäººå·¥å†³ç­– ===
            approval = input(
                "\n[ç®¡ç†å‘˜]: æ˜¯å¦æ‰¹å‡†æ­¤æ“ä½œ? (y/n/e[ç¼–è¾‘]): ").strip().lower()

            if approval == 'y':
                # === æ‰¹å‡†æ“ä½œ ===
                print("\n[ç³»ç»Ÿ]: âœ… æ“ä½œå·²æ‰¹å‡†ï¼Œç»§ç»­æ‰§è¡Œ...")

                # ç»§ç»­æ‰§è¡Œä»£ç†
                hitl_response = HITLResponse(
                    decisions = [ApproveDecision(type = "approve")]  # æ‰¹å‡†å†³ç­–
                )

                for event in agent.stream(
                    Command(resume = hitl_response),
                    config = config,
                    stream_mode = "values"
                ):
                    if "messages" in event:
                        last_msg = event["messages"][-1]
                        if last_msg.type == "tool":
                            print(f"[å·¥å…·è¿”å›]: {last_msg.content}")
                        elif last_msg.type == "ai" and last_msg.content:
                            print(f"[AI æœ€ç»ˆå›ç­”]: {last_msg.content}")

            elif approval == 'e':
                # === ç¼–è¾‘æ“ä½œ ===
                print("\n[ç³»ç»Ÿ]: âœï¸  ç¼–è¾‘æ¨¡å¼...")
                print(f"å½“å‰å‚æ•°: {tool_call['args']}")

                new_query = input(f"current query [{tool_call['args'].get('query', '')}], enter new query or press Enter to keep: ").strip()
                new_category = input(f"current data_category [{tool_call['args'].get('data_category', 'confidential')}], enter new category or press Enter to keep: ").strip()

                updated_args = tool_call['args'].copy()
                if new_query:
                    updated_args['query'] = new_query
                if new_category:
                    updated_args['data_category'] = new_category

                print(f"\n[ç³»ç»Ÿ]: ä½¿ç”¨æ›´æ–°åçš„å‚æ•°ç»§ç»­æ‰§è¡Œ...")
                print(f"æ›´æ–°åçš„å‚æ•°: {updated_args}")

                hitl_response = HITLResponse(
                    decisions = [
                        EditDecision(
                            type = "edit",
                            edited_action = {
                                "name": tool_call['name'],
                                "args": updated_args
                            }
                        )
                    ]
                )

                for event in agent.stream(
                        Command(resume = hitl_response),
                        config = config,
                        stream_mode = "values"
                ):
                    if "messages" in event:
                        last_msg = event["messages"][-1]
                        if last_msg.type == "tool":
                            print(f"\n[å·¥å…·è¾“å‡º]:\n{last_msg.content}")
                        elif last_msg.type == "ai" and last_msg.content:
                            print(f"\n[AI æœ€ç»ˆå›å¤]: {last_msg.content}")

            else:
                # === æ‹’ç»æ“ä½œ ===
                print("\n[ç³»ç»Ÿ]: âŒ æ“ä½œè¢«æ‹’ç»")

                rejection_reason = input(
                    "æ‹’ç»åŸå›  (å¯é€‰): ").strip() or "æ“ä½œè¢«ç®¡ç†å‘˜æ‹’ç»ï¼Œæƒé™ä¸è¶³"

                hitl_response = HITLResponse(
                    decisions = [RejectDecision(
                        type = "reject",
                        message = rejection_reason
                    )]
                )

                for event in agent.stream(
                        Command(resume = hitl_response),
                        config = config,
                        stream_mode = "values"
                ):
                    if "messages" in event:
                        last_msg = event["messages"][-1]
                        if last_msg.type == "ai" and last_msg.content:
                            print(f"\n[AI å›å¤]: {last_msg.content}")
                        elif last_msg.type == "tool":
                            print(f"\n[å·¥å…·æ¶ˆæ¯]: {last_msg.content}")

                print("\n[ç³»ç»Ÿ]: æµç¨‹å·²ç»ˆæ­¢")

        else:
            print("âš ï¸  æ²¡æœ‰æ£€æµ‹åˆ°å¾…å¤„ç†çš„å·¥å…·è°ƒç”¨")
    else:
        print("â„¹ï¸  æµç¨‹å·²å®Œæˆï¼Œæ²¡æœ‰è§¦å‘ä¸­æ–­")
        if snapshot.values.get("messages"):
            last_msg = snapshot.values["messages"][-1]
            if last_msg.type == "ai" and last_msg.content:
                print(f"\n[æœ€ç»ˆå›å¤]: {last_msg.content}")

    print("\n" + "=" * 70)
    print("âœ… HITL æµ‹è¯•å®Œæˆï¼")
    print("=" * 70)

    # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
    print("\nğŸ“Š ä¸­é—´ä»¶ç»Ÿè®¡ä¿¡æ¯:")
    logging_middleware.logger.print_statistics()

#run_hitl_interactive_test()


#%%
def run_normal_rag_test():
    """
    è¿è¡Œæ™®é€š RAG æ£€ç´¢æµ‹è¯•
    æµ‹è¯• query_retrieval_knowledge å·¥å…·çš„æ£€ç´¢æµç¨‹
    """
    print("\n" + "="*70)
    print("ğŸš€ å¼€å§‹æ‰§è¡Œæ™®é€š RAG æ£€ç´¢æµ‹è¯•")
    print("="*70)

    # æµ‹è¯•æç¤ºè¯ï¼šè§¦å‘ LangChain çŸ¥è¯†åº“æ£€ç´¢
    test_queries = [
        "LangChain ä¸­çš„ Agent æ˜¯ä»€ä¹ˆï¼Ÿå®ƒæœ‰å“ªäº›æ ¸å¿ƒç»„ä»¶ï¼Ÿ",
        "å¦‚ä½•åœ¨ LangChain ä¸­ä½¿ç”¨ RAG è¿›è¡Œé—®ç­”ï¼Ÿ",
        "LangGraph å’Œ LangChain æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ"
    ]

    print("\nå¯ç”¨çš„æµ‹è¯•é—®é¢˜ï¼š")
    for i, query in enumerate(test_queries, 1):
        print(f"{i}. {query}")

    choice = input("\nè¯·é€‰æ‹©æµ‹è¯•é—®é¢˜ (1-3) æˆ–è¾“å…¥è‡ªå®šä¹‰é—®é¢˜: ").strip()

    if choice.isdigit() and 1 <= int(choice) <= len(test_queries):
        user_input = test_queries[int(choice) - 1]
    else:
        user_input = choice if choice else test_queries[0]

    print(f"\n[ç”¨æˆ·]: {user_input}")
    print("\n[ç³»ç»Ÿ]: å¼€å§‹å¤„ç†è¯·æ±‚...\n")

    # ä½¿ç”¨æ–°çš„ thread_id é¿å…ä¸ HITL æµ‹è¯•å†²çª
    rag_config = {"configurable": {"thread_id": "rag-test-thread"}}

    # ç”¨äºè·Ÿè¸ªå·²æ‰“å°çš„æ¶ˆæ¯ï¼Œé¿å…é‡å¤
    printed_message_ids = set()

    # æ‰§è¡Œ Agent æµç¨‹
    for event in agent.stream(
        {"messages": [{"role": "user", "content": user_input}]},
        config=rag_config,
        stream_mode="values",
        context={"user_role": "å¼€å‘è€…"}
    ):
        if "messages" in event:
            last_msg = event["messages"][-1]

            # ä½¿ç”¨æ¶ˆæ¯ ID æ¥é¿å…é‡å¤æ‰“å°
            msg_id = getattr(last_msg, 'id', None)
            if msg_id and msg_id in printed_message_ids:
                continue

            if msg_id:
                printed_message_ids.add(msg_id)

            # æ˜¾ç¤º AI çš„æ€è€ƒè¿‡ç¨‹
            if last_msg.type == "ai":
                if hasattr(last_msg, 'tool_calls') and last_msg.tool_calls:
                    tool_call = last_msg.tool_calls[0]
                    print(f"ğŸ¤– [AI å†³ç­–]: è°ƒç”¨å·¥å…· -> {tool_call['name']}")
                    print(f"   å‚æ•°: {tool_call.get('args', {})}")
                elif last_msg.content:
                    print(f"\nğŸ’¬ [AI å›å¤]:\n{last_msg.content}")

            # æ˜¾ç¤ºå·¥å…·æ‰§è¡Œç»“æœ
            elif last_msg.type == "tool":
                tool_name = getattr(last_msg, 'name', 'unknown')
                print(f"\nğŸ”§ [å·¥å…·æ‰§è¡Œ]: {tool_name}")
                print(f"ğŸ“„ [æ£€ç´¢ç»“æœ]:\n{'-'*70}")
                # åªæ˜¾ç¤ºå‰500ä¸ªå­—ç¬¦ï¼Œé¿å…è¾“å‡ºè¿‡é•¿
                content = last_msg.content
                if len(content) > 500:
                    print(f"{content[:500]}...\n(ç»“æœå·²æˆªæ–­ï¼Œå…± {len(content)} å­—ç¬¦)")
                else:
                    print(content)
                print(f"{'-'*70}\n")

    print("\n" + "="*70)
    print("âœ… æ™®é€š RAG æ£€ç´¢æµ‹è¯•å®Œæˆï¼")
    print("="*70)

    # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
    print("\nğŸ“Š ä¸­é—´ä»¶ç»Ÿè®¡ä¿¡æ¯:")
    logging_middleware.logger.print_statistics()

run_normal_rag_test()



