import os
import asyncio
from typing import Optional, Set
from model_factory import get_model
from deepagents import create_deep_agent
from deepagents.backends import FilesystemBackend
from langgraph.checkpoint.memory import InMemorySaver
from dotenv import load_dotenv
from langchain_tavily import TavilySearch
from langchain.agents.middleware.human_in_the_loop import (
    HITLResponse,
    ApproveDecision,
    EditDecision,
    RejectDecision
)
from langgraph.types import Command
from langchain_core.messages import BaseMessage, ToolMessage, AIMessage

load_dotenv(override=True)

async def run_interrupt_test():
    """
    ç¤ºä¾‹ 1: åŸºç¡€ä¸­æ–­åŠŸèƒ½ (å°è£…ç‰ˆ)
    åœ¨å·¥å…·è°ƒç”¨å‰ä¸­æ–­ï¼Œè®©ç”¨æˆ·ç¡®è®¤æ˜¯å¦ç»§ç»­æ‰§è¡Œ
    """
    print("\n" + "="*80)
    print("ğŸ“š ç¤ºä¾‹ 1: interrupt_on ä½¿ç”¨")
    print("="*80)
    print("\nåŠŸèƒ½ï¼šåœ¨å·¥å…·è°ƒç”¨å‰æš‚åœï¼Œç­‰å¾…äººå·¥ç¡®è®¤\n")

    # åˆ›å»º LLM å’Œå·¥å…·
    llm = get_model()
    search_tool = TavilySearch(max_results=2)

    # åˆ›å»º Agentï¼Œè®¾ç½®åœ¨ "tools" èŠ‚ç‚¹ä¸­æ–­
    agent = create_deep_agent(
        model=llm,
        tools=[search_tool],
        backend=FilesystemBackend(root_dir="./workspace",virtual_mode=True),
        checkpointer=InMemorySaver(),  # å¿…éœ€ï¼ç”¨äºæ”¯æŒä¸­æ–­å’Œæ¢å¤
        interrupt_on={"tavily_search": True},  # åœ¨ç‰¹å®šå·¥å…·è°ƒç”¨æ—¶ä¸­æ–­
    )

    # å®šä¹‰ä»»åŠ¡
    task = "æœç´¢ 'Python å¼‚æ­¥ç¼–ç¨‹' çš„æœ€æ–°ä¿¡æ¯ï¼Œå¹¶åˆ›å»ºä¸€ä¸ªæ€»ç»“æ–‡ä»¶"

    # é…ç½®ä¼šè¯ ID
    # ä¸ºäº†é¿å…ä¹‹å‰çš„çŠ¶æ€å¹²æ‰°ï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸€ä¸ªæ–°çš„ thread_id
    config = {"configurable": {"thread_id": "demo_basic_refactored_v1"}}

    print(f"ğŸ“‹ ä»»åŠ¡: {task}\n")
    print("ğŸš€ å¼€å§‹æ‰§è¡Œ...\n")

    # è¿½è¸ªå·²æ‰“å°çš„æ¶ˆæ¯æ•°é‡ï¼Œé¿å…é‡å¤æ‰“å°
    message_history_len = 0

    # --- ç¬¬ä¸€æ¬¡æ‰§è¡Œ ---
    print("ã€ç¬¬ä¸€æ¬¡æ‰§è¡Œ - é¢„æœŸä¼šä¸­æ–­ã€‘")
    async for event in agent.astream({"messages": [("user", task)]}, config=config):
        if "messages" in event:
            current_messages = event["messages"]
            if len(current_messages) > message_history_len:
                # æ‰“å°æ–°å¢çš„æ¶ˆæ¯
                for i in range(message_history_len, len(current_messages)):
                    msg = current_messages[i]
                    if msg.type == "ai":
                        if hasattr(msg, 'tool_calls') and msg.tool_calls:
                            print(f"ğŸ”§ AI å†³å®šè°ƒç”¨å·¥å…·: {msg.tool_calls[0]['name']}")
                            print(f"   å‚æ•°: {msg.tool_calls[0]['args']}")
                        elif msg.content:
                            print(f"ğŸ’¬ AI: {msg.content}")
                    elif msg.type == "tool":
                        print(f"âœ… å·¥å…·è¾“å‡º: {msg.content[:100]}..." if len(msg.content) > 100 else f"âœ… å·¥å…·è¾“å‡º: {msg.content}")

                message_history_len = len(current_messages)

    # æ£€æŸ¥æ˜¯å¦ä¸­æ–­
    # ä½¿ç”¨ aget_state (async) è·å–çŠ¶æ€
    state = await agent.aget_state(config)
    print(f"\nâ¸ï¸  æ‰§è¡ŒçŠ¶æ€: {state.next}")

    if state.tasks:
        print(f"\n--- ğŸ›‘ æ‰§è¡Œå·²æš‚åœ (HITL Middleware) ---")
        print(f"ä¸‹ä¸€æ­¥éª¤ (Next): {state.next}")

        last_message = state.values["messages"][-1]

        if hasattr(last_message, "tool_calls") and last_message.tool_calls:
            tool_call = last_message.tool_calls[0]
            print(f"\n[å¾…å®¡æ‰¹æ“ä½œ]:")
            print(f"  - å·¥å…·: {tool_call['name']}")
            print(f"  - å‚æ•°: {tool_call['args']}")

            # === äººå·¥ä»‹å…¥ ===
            approval = input("\n[ç®¡ç†å‘˜]: æ˜¯å¦æ‰¹å‡†æ‰§è¡Œæ­¤æ“ä½œ? (y/n/e[ç¼–è¾‘]): ")

            if approval.lower() == 'y':
                print("\n[ç³»ç»Ÿ]: æ“ä½œå·²æ‰¹å‡†ï¼Œç»§ç»­æ‰§è¡Œ...")

                hitl_response = HITLResponse(
                    decisions=[ApproveDecision(type="approve")]
                )

                # === æ¢å¤æ‰§è¡Œ ===
                # ä½¿ç”¨ Command(resume=...)
                async for event in agent.astream(
                    Command(resume=hitl_response),
                    config=config,
                    stream_mode="values"
                ):
                    if "messages" in event:
                        current_messages = event["messages"]
                        if len(current_messages) > message_history_len:
                            for i in range(message_history_len, len(current_messages)):
                                msg = current_messages[i]

                                # ä¼˜åŒ–æ‰“å°é€»è¾‘ï¼Œæ¸…æ™°å±•ç¤º AI å›å¤
                                if msg.type == "tool":
                                    print(f"\n[å·¥å…·è¾“å‡º]:\n{msg.content[:300]}..." if len(msg.content) > 300 else f"\n[å·¥å…·è¾“å‡º]:\n{msg.content}")
                                elif msg.type == "ai":
                                    if msg.content:
                                        print(f"\n[AI å›å¤]:\n{msg.content}\n")
                                    elif msg.tool_calls:
                                        print(f"\nğŸ”§ AI å†³å®šè°ƒç”¨å·¥å…·: {msg.tool_calls[0]['name']}")
                                        print(f"   å‚æ•°: {msg.tool_calls[0]['args']}")

                            message_history_len = len(current_messages)

            else:
                print("\n[ç³»ç»Ÿ]: æ“ä½œè¢«æ‹’ç»æˆ–æ‚¨é€‰æ‹©äº†å…¶ä»–é€‰é¡¹ (æœ¬æ¼”ç¤ºä»…å¤„ç† 'y')ã€‚")
    else:
        print("æµç¨‹å·²å®Œæˆï¼Œæ²¡æœ‰è§¦å‘ä¸­æ–­ã€‚")
        if state.values.get("messages"):
            last_msg = state.values["messages"][-1]
            if last_msg.type == "ai" and last_msg.content:
                print(f"\n[æœ€ç»ˆå›å¤]: {last_msg.content}")

if __name__ == "__main__":
    try:
        asyncio.run(run_interrupt_test())
       # await run_interrupt_test()
    except KeyboardInterrupt:
        print("\nç¨‹åºå·²åœæ­¢")
